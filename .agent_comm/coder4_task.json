{
  "agent_id": "coder4",
  "task_id": "task_5",
  "files": [
    {
      "name": "README.md",
      "purpose": "Project documentation",
      "priority": "medium"
    },
    {
      "name": "utils.py",
      "purpose": "Utility functions",
      "priority": "low"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.MA_2507.22278v1_Successor_Features_for_Transfer_in_Alternating_Mar",
    "project_type": "agent",
    "description": "Enhanced AI project based on cs.MA_2507.22278v1_Successor-Features-for-Transfer-in-Alternating-Mar with content analysis. Detected project type: agent (confidence score: 16 matches).",
    "key_algorithms": [
      "Continue",
      "Ggpi",
      "Iteration",
      "Lum",
      "Structured",
      "Large-Scale",
      "Learning",
      "Exact",
      "Ment",
      "Compared"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.MA_2507.22278v1_Successor-Features-for-Transfer-in-Alternating-Mar.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nSuccessor Features for Transfer in Alternating Markov Games\nSunny Amatya1, Yi Ren2, Zhe Xu2, and Wenlong Zhang1\u2217\nAbstract \u2014 This paper explores successor features for knowl-\nedge transfer in zero-sum, complete-information, and turn-\nbased games. Prior research in single-agent systems has shown\nthat successor features can provide a \u201cjump start\u201d for agents\nwhen facing new tasks with varying reward structures. How-\never, knowledge transfer in games typically relies on value and\nequilibrium transfers, which heavily depends on the similarity\nbetween tasks. This reliance can lead to failures when the tasks\ndiffer signi\ufb01cantly. To address this issue, this paper presents\nan application of successor features to games and presents a\nnovel algorithm called Game Generalized Policy Improvement\n(GGPI), designed to address Markov games in multi-agent\nreinforcement learning. The proposed algorithm enables the\ntransfer of learning values and policies across games. An upper\nbound of the errors for transfer is derived as a function\nthe similarity of the task. Through experiments with a turn-\nbased pursuer-evader game, we demonstrate that the GGPI\nalgorithm can generate high-reward interactions and one-shot\npolicy transfer. When further tested in a wider set of initial\nconditions, the GGPI algorithm achieves higher success rates\nwith improved path ef\ufb01ciency compared to those of the baseline\nalgorithms.\nI. I NTRODUCTION\nMulti-agent reinforcement learning (MARL) is a powerful\nframework for autonomous robots to coordinate in dynamic\nenvironments [1]. From multi-robot collaboration to compet-\nitive adversarial tasks, MARL has demonstrated emergent\nbehaviors that scale to complex tasks like multi agent hide-\nand-seek and traf\ufb01c signal control [2], [3]. However, despite\nits success, MARL faces fundamental challenges in trans-\nferability\u2014agents often require extensive retraining to adapt\nthe learned policies to new tasks or agents [4], [5]. This\nlimitation signi\ufb01cantly hinders real-world deployment, where\nadaptability and ef\ufb01ciency are crucial.\nExisting transfer learning approaches, such as curricu-\nlum learning and source task selection, require training\non multiple tasks before achieving effective policy transfer\n[6], [7]. While these methods improve adaptation, they are\ncomputationally expensive and lack \ufb02exibility for robotic\napplications requiring humans to train in all possible setting.\nOther approaches focus on adapting to opponent behaviors\n[8], but they are inherently reactive and fail to generalize\nto rapidly changing interactions. Additionally, many MARL\ntransfer methods do not provide convergence guarantees,\nThis work was supported by the National Science Foundation under Grant\nCMMI-1925403.\n1S. Amatya and W. Zhang are with the School of Manufacturing Systems\nand Networks, Arizona State University, Mesa, AZ, 85212, USA. Email:\n{samatya,wenlong.zhang }@asu.edu\n2Y . Ren, and Z. Xu are with the School for Engineering of Matter,\nTransport, and Energy, Arizona State University, Tempe, AZ, 85287, USA.\nEmail:{yiren,xzhe1 }@asu.edu\n* Address all correspondence to this author.making them less applicable for mission-critical tasks [9].\nTo address these challenges, we propose a structured transfer\nlearning approach in turn-based Markov Games, which pro-\nvide a principled framework for robust policy transfer. Our\nmethod integrates Successor Features (SF), a novel Game\nbased Generalized Policy Improvement (GPI) with MARL\nto ensure stable adaptation and ef\ufb01cient value transfer [10].\nUnlike prior SF-based methods [11], our approach leverages\nthe alternating structure of Markov Games, allowing agents\nto anticipate and optimize against their opponent\u2019s strategy.\nWe demonstrate our approach in a minmax setting. This for-\nmulation enables transfer in adversarial settings, accounting\nfor the worst-case opponent behavior and with theoretical\nconvergence guarantees, making it particularly well-suited\nfor multi-agent applications.\nContributions : In this work, we introduce a novel transfer\nlearning algorithm for MARL. Speci\ufb01cally, the contributions\nof this paper are\n\u2022Developing an SF-based minmax Q-learning algorithm\nthat integrates novel Game based GPI for policy transfer\nin turn-based Markov Games.\n\u2022Performing convergence analysis, ensuring stable learn-\ning in competitive multi-agent settings.\n\u2022Empirical evaluation of our method in a two-player,\nminmax, turn-based pursuer-evader game, comparing it\nwith ablations and baseline algorithms to demonstrate\nvalue transfer and ef\ufb01ciency of the new policy.\nThe remainder of this paper is organized as follows: In\nSection II, related works are reviewed. The preliminaries\nrequired for is highlighted in Section III. The proposed\nmethod is described in Section IV. Experimental results are\ngiven in Section V and Section VI concludes this paper.\nII. R ELATED WORK\nTransfer in Games. One idea to transfer learning in games\nis to reuse past experiences , where relevant features from\nprevious games are leveraged to improve performance in new\nones. Examples include General Game Playing (GGP) [12]\nand Case-Based Reinforcement Learning (RL) [13], where\nagents build knowledge from past game play. However, these\nsimilarity-based methods require a substantial amount of\nexperience to construct a useful base case, and ensuring\nconvergence remains an open challenge. A more structured\napproach is value function transfer , which uses prior knowl-\nedge or shared representations to facilitate adaptation across\ntasks. An example is the Universal Value Function (UVF)\n[14] and value function transfer in multi-agent settings [15].\nHowever, these methods struggle when reward structures or\nagent interaction dynamics vary signi\ufb01cantly across games,arXiv:2507.22278v1  [cs.MA]  29 Jul 2025\n\n--- Page 2 ---\nlimiting their generalizability [16]. Another policy transfer\nmethod is equilibrium transfer , which accelerates learning\nby reusing policies from previous games in new ones [17].\nThis approach relies on identifying similarities in the goals\nof the games, allowing the agent to adapt strategies more\nef\ufb01ciently. However, its effectiveness is limited when the\ngoals or reward structures differ signi\ufb01cantly, making one-\nshot policy transfer impractical in such cases. In this paper,\nwe develop a framework for value and policy transfer in\ngames with convergence guarantees, using similarity-based\nmethods while avoiding the burden of building a library of\nexperiences.\nSuccessor Features for Transfer in Reinforcement Learn-\ning. In transfer learning, SFs are used to model each task\nby de\ufb01ning the reward function [10]. The ability to transfer\nknowledge across tasks stems from a generalization of two\ncore RL operations: policy evaluation and policy improve-\nment. These generalized operations, known as \u201cgeneralized\npolicy updates\u201d, extend the standard point-based methods to\nset-based approaches, enabling the reuse of task solutions.\nWhen the reward function of a task can be approximated as\na linear combination of reward functions for other tasks, RL\ncan be simpli\ufb01ed to a linear regression problem, solvable\nwith a fraction of the data as compared to baseline Q-\nlearning [18]. Additionally, the Generalized Policy Improve-\nment (GPI) algorithm allows agents to generate policies from\nreference tasks by selecting the best actions accordingly.\nBuilding on these generalized policy updates, SFs have\nbeen applied in various RL settings, including risk-sensitive\nenvironments [19] and dissimilar tasks [20]. While earlier\nwork focused on agents with a single goal but different pref-\nerences, this approach has been extended to novel and unseen\ngoals [10]. Recent studies have explored SF-based transfer in\nMARL, showing that training with SFs accelerates learning\nby leveraging previously acquired knowledge [11]. However,\nthese studies have mainly focused on cooperative agents\nwhere all agents focus on single cumulative reward. In this\npaper, we extend the use of SFs to a new setting\u2014transfer\nlearning in games where two agents compete in a turn-based,\nzero-sum Markov game.\nIII. P RELIMINARIES AND PROBLEM FORMULATION\nIn this section, we give a brief background of SFs, and\ntwo-player zero-sum Markov games.\nA. Successor Features\nIn Generalized Policy Evaluation (GPE), for a policy \u00c3, it\ntakes the task ras input and outputs the value function Q\u03c0\nr.\nThe features for a given state and action are \u03d5:S\u00d7A\u2192Rd.\nFor any feature weight, w\u2208Rd, a task is de\ufb01ned using the\nreward function as\nrw(s,a) =\u03d5(s,a)Tw (1)\nwhere\u00b7Tdenotes the transpose of the vector. Following the\nde\ufb01nition in [10], the SFs of a policy \u00c3is de\ufb01ned as:\n\u00c8\u03c0(s,a)T=E\u03c0[T/summationdisplay\nt=0\u00b5t\u03d5(st+1,at+1)|st=s,at=a](2)Hence,\u00c8\u03c0(s,a)gives the expected discounted sum of \u03d5\nwhen following the policy \u00c3starting from (s,a). Thus\u00c8\u03c0\ncan be seen as a d-dimensional value function in which the\nfeatures\u03d5(s,a)play role in reward functions.\nGiven\u00c8\u03c0, one can quickly evaluate \u00c3on a taskrwby\ncomputing\n\u00c8\u03c0(s,a)Tw=E\u03c0/bracketleftbigg\u221e/summationdisplay\nt=0\u00b5t\u03d5(st+1,at+1)Tw/vextendsingle/vextendsingle/vextendsingle/vextendsinglest=s,at=a/bracketrightbigg\n=E\u03c0/bracketleftbigg\u221e/summationdisplay\nt=0\u00b5tr(st+1,at+1)/vextendsingle/vextendsingle/vextendsingle/vextendsinglest=s,at=a/bracketrightbigg\n=Q\u03c0\nr(s,a)\n(3)\nAs a consequence, SFs satisfy the Bellman equation,\nwhich means they can be computed using standard RL\nmethods like temporal difference learning.\nB. Two-player Zero-sum Markov Game\nIn this paper, we consider a two-player, minmax Markov\ngame, where two agents (ego and other) alternately take\nactions to maximize or minimize the cumulative discounted\nreward, respectively. In alternating Markov games, since the\nagents take turns to make decisions, the other agent has the\nadvantage of observing the ego\u2019s action before responding\nwhile the ego agent has the \ufb01rst mover\u2019s advantage.\nFollowing the de\ufb01nition in [21], in an alternating Markov\ngame, the state-space is S, the action-space of the ego is\nA, and the action-space of the other is B. The ego selects\nan actiona\u2208Aat the current state, and the other can\nobserve the ego\u2019s action a, and selects its decision b\u2208B.\nThen, the system jumps to the next states with probability\nP(s\u2032|s,a,b), and the transition incurs with reward r(s,a,b).\nFor convenience, we consider a deterministic reward function\nand simply write r(st,at,bt) =rt,t\u2208{0,1,...}.The ego\u2019s\nstationary deterministic policy, \u00c3:S\u2192A, maps a state\ns\u2208Sto an action \u00c3(s)\u2208A, while the other\u2019s stationary\ndeterministic policy, \u00b5:S\u00d7A\u2192B, maps a state s\u2208Sand\nthe ego\u2019s action a\u2208Ato an action \u00b5(s,a)\u2208B. It has been\nshown that there exists an optimal stationary deterministic\npolicy for both the ego and other agents [22].\nThe objective of the Markov game is to determine the\nego\u2019s optimal policy, denoted as \u00c3\u2217, and the optimal adver-\nsarial policy, denoted as \u00b5\u2217:\n(\u00c3\u2217,\u00b5\u2217) :=argmax\n\u03c0\u2208\u0398min\n\u00b5\u2208\u2126E/bracketleftbig/summationtext\u221e\nt=0\u00b5trt|\u00c3,\u00b5/bracketrightbig\n(4)\nwhere\u00b5\u2208[0,1)is the discount factor, \u0398and\u2126are\nthe sets of all admissible deterministic policies of the ego\nand the other agent, respectively. (s0,a0,b0,s1,a1,b1,...)\nis a state-action trajectory generated under policies \u00c3,\u00b5,\nandE[/summationtext\u221e\nt=0\u00b5jrt|\u00c3,\u00b5]is an expectation conditioned on the\npolicies\u00c3and\u00b5, which looks similar to (3). Thus, we\npropose to decompose the Q-values into the SF values, as\nhighlighted in the Sec. IV-B.\nSome fundamental tools used in Markov decision pro-\ncesses, such as the value function and Bellman equation,\n\n--- Page 3 ---\ncan be also applied to the two-player Markov game [23]. In\nparticular, the optimal Q-function is de\ufb01ned as\nQ\u2217(s,a,b) := max\n\u03c0\u2208\u0398min\n\u00b5\u2208\u2126E/bracketleftbigg/summationtext\u221e\nt=0\u00b5trt|st=s,at=a,\nbt=b,\u00c3,\u00b5/bracketrightbigg\n(5)\nwhich satis\ufb01es the optimal Q-Bellman equation\nQ\u2217(s,a,b) :=r(s,a,b)+\n\u00b5/summationdisplay\ns\u2032\u2208SP(s\u2032|s,a,b)max\na\u2032\u2208Amin\nb\u2032\u2208BQ\u2217(s,a\u2032,b\u2032)(6)\nThe ego\u2019s stationary optimal policy is given by\n\u00c3\u2217(s) =argmax\na\u2208Amin\nb\u2208BQ\u2217(s,a,b), (7)\nand the other\u2019s stationary optimal policy is\n\u00b5\u2217(s,a) =argmin\nb\u2208BQ\u2217(s,a,b). (8)\nUsing the Bellman equation, the Q-value iteration (Q-VI) is\nthe recursion\nQt+1(s,a,b) = (FQt)(s,a,b),\u2200(s,a,b)\u2208S\u00d7A\u00d7B,\n(9)\nwhereFis the contraction operator. It is known that the Q-\nVI converges to Q\u2217[24]. Let \u03a0 ={\u00c3,\u00b5}for the sake of\nease moving forward.\nIV. T RANSFER VIA SUCCESSOR FEATURES\nWe now consider scenarios where the source and target\ntasks have distinct reward functions. The goal of transfer in\nMARL is to learn an optimal policy for the target task by\nutilizing knowledge gained from the source task. We \ufb01rst\nde\ufb01ne the set of source task models using (1) as follows\nM\u03c6={M(S,A,B,p,r,\u00b5 )|r(s,a,b) =\u03d5(s,a,b)Tw}.\n(10)\nAt the time when the agent wants to learn a target task, it\nalready has a collection of good policies for a set of source\ntasks inM={M1,M2,...,M n}withMi\u2208M\u03c6. Hence for\na new taskMn+1, we are able to get a value function utilizing\nwn+1. This value generation is before we have carried out\nany policy improvement in the target task. Let the initial\npolicy that the agent computes be \u03a0\u2032. At this point, we can\nobserve one-shot transfer. In our experiments, we will be\nable to show this as well. If we continue learning in the new\ntask, the resulting policy \u03a0should be such that Q\u03a0(s,a,b)\u2265\nQ\u03a0\u2032(s,a,b),\u2200(s,a,b)\u2208S\u00d7A\u00d7B.\nWith this setup, we can now describe our approach for\nsolving the transfer problem outlined in (10). Our solution\nis presented in two stages: we \ufb01rst generalize policy im-\nprovement for games, and then demonstrate how SFs can\nbe effectively utilized to implement this generalized policy\nimprovement.A. Game Generalized Policy Improvement\nIn this section, we highlight our proposed policy improve-\nment algorithm for games, and explain how the new policy is\ncomputed based on the Q-value of a set of policies. We show\nthat this extension can be a natural greedy policy for both\nagents over the available aforementioned Q-value functions.\nGenerally for a single-task multi-agent game, the policy\niteration algorithm initially chooses an arbitrary pair of\nstrategies and repeatedly selects a better pair of strategies\nuntil a local optimal pair of strategies is found. This is\nfurther highlighted in Sec. III-B where both agents try to\n\ufb01nd their optimal policy by choosing greedy policies in the\navailableQvalues [25]. For multiple tasks, we propose Game\nGeneralized Policy Improvement (GGPI) as follows.\nTheorem 1. (Game Generalized Policy Improvement) .\nLet\u03a01,\u03a02,...,\u03a0nbenpolicies and let \u02dcQ\u03a01,\u02dcQ\u03a02,...,\u02dcQ\u03a0n\nbe approximations of their respective Qfunctions such that\n(11)|Q\u03a0i(s,a,b)\u2212\u02dcQ\u03a0i(s,a,b)|\u2264\u03f5\n\u2200s\u2208S,a\u2208A,b\u2208Bandi\u2208{1,2,...n}.\nDe\ufb01ne\u00c3GGPI(s)\u2208argmaxaminbmini\u02dcQ\u03a0i(s,a,b), one\ncan show\nQ\u03a0(s,a,b)\u2265min\niQ\u03a0i(s,a,b)\u22122\u03f5\n1\u2212\u00b5. (12)\nThe proof is provided in Supplementary Materials I1. Our\ntheorem accounts for cases where the value functions of\npolicies are not computed exactly, either due to the use of\nfunction approximation or because an exact algorithm has not\nbeen fully executed. This approximation error is captured in\n(11), with\u03f5which appears as a penalty term in the lower\nbound (12). As stated in [10], such a penalty is inherent\nto the use of approximation in RL and is identical to the\npenalty incurred in the single-policy case (Proposition 6.1 in\n[26]). In this context, Theorem 1 guarantees that the selected\npolicy\u03a0will perform no worse than any of the policies\n\u03a01,\u03a02,...,\u03a0n. This result extends naturally to multi-agent\nMarkov games, where the policy of an agent, as de\ufb01ned in\n(7), is generalized to a set of policies across agents.\nIn this work, we analyze two key aspects: 1) how learning\nnew tasks and the use of SFs facilitate the transfer process,\nand 2) the analysis of multiple policies across different initial\nconditions to enable effective transfer using SF.\nB. Game Generalized Policy Improvement with Successor\nFeature\nWe start this section by extending our notation slightly to\nmake it easier to refer to the quantities involved in transfer\nlearning. Let Mibe a task inM\u03c6de\ufb01ned by wi\u2208Rd. We\nuse\u03a0\u2217\nito refer to an optimal policy of Miand useQ\u03a0\u2217\ni\nito\nrefer to its value function. The value function of \u03a0\u2217\niwhen\nexecuted in Mj\u2208M\u03c6will be denoted by Q\u03a0\u2217\ni\nj.\nSuppose now that an agent has computed optimal\npolicies for the tasks M1,M2,...,M n\u2208 M\u03c6. When\n1Supplementary materials can be found at https://home.riselab.\ninfo/downloads/IROS_2025_SuppMaterials.pdf\n\n--- Page 4 ---\npresented with a new task Mn+1, the agent computes\n{Q\u03a0\u2217\n1\nn+1,Q\u03a0\u2217\n2\nn+1,...,Q\u03a0\u2217\nn\nn+1}, the evaluation of each \u03a0\u2217\niunder\nthe new reward function induced by wn+1.\nSuppose that we have learned the functions Q\u03a0\u2217\ni\niusing the\nrepresentation shown in (3). Now, if the reward changes to\nrn+1(s,a,b,s\u2032) =\u03d5(s,a,b,s\u2032)Twn+1, as long as we have\nwn+1, we can compute the new value function of \u03a0iby\nsimply making Q\u03a0\u2217\ni\nn+1(s,a,b) =\u00c8\u03a0\u2217\ni(s,a,b)Twn+1. This\nsigni\ufb01cantly simpli\ufb01es the computation of all Q\u03c0\u2217\ni\nn+1.\nOnce the functions Q\u03c0\u2217\ni\nn+1have been computed, we can\napply GPI to derive a policy \u03a0whose performance on Mn+1\nis no worse than the performance of \u03a0\u2217\n1,\u03a0\u2217\n2,...,\u03a0\u2217\nnon the\nsame task. A question that arises in this case is whether we\ncan provide stronger guarantees on the performance of \u03a0\nby exploiting the structure shared by the tasks in M\u03c6. The\nfollowing Lemma answers this question in the af\ufb01rmative.\nLemma 1. Let\u00b6ij= max s,a,b|ri(s,a,b)\u2212rj(s,a,b)|and\nlet\u03a0be an arbitrary policy. Then,\n|Q\u03a0\ni(s,a,b)\u2212Q\u03a0\nj(s,a,b)|\u2264\u00b6ij\n1\u2212\u00b5. (13)\nThe detailed proofs along with additional information\ncan be found in the Supplementary Materials. This lemma\nprovides an upper bound of the differences in action values\nof the transfer policy in (13) and the optimal policies of the\ntarget taskMj. As stated in [10], this provides the guidance\nfor application of the algorithm for storing and removing the\nSFs. It further captures the role of the reward rwhich can\nbe further broken down into feature \u03d5and the weight w.\nHence, one can show \u00b6ij= max s,a,b|\u03d5||wr,i\u2212wr,j|, where\nthe weights characterize the difference in the task objective.\nAlgorithm and practical use. Algorithm 1 outlines our\nsolution using turn-based minmax Q-learning as the baseline\nRL method. In this approach, both the ego agent and the\nother agent perform a one-step lookahead into their partner\u2019s\nactions, choosing moves to either maximize or minimize\nrewards based on the opponent\u2019s choices. Our algorithm\nhighlights the use of GGPI by the ego and other agents,\nas shown in lines 7 and 18, respectively. This allows the\npolicies\u03a0ito be used to solve each task Mi.\nV. E XPERIMENTS\nIn this section, we \ufb01rst introduce the testing setup which\nincludes the environment de\ufb01nitions, experimental condi-\ntions, evaluation metrics and the baselines. Then we show the\ncomparison of our algorithm with the baseline algorithms.\nA. Simulation Setup\n1)Environment de\ufb01nition :The experiments are con-\nducted using a pursuer-evader game on a 5\u00d75grid, as shown\nin Fig. 2. The evader/ego agent is represented by a robber, the\npursuer/other agent by a police of\ufb01cer, and the goal position\nby an exit door. The action space includes four possible\nactions,A={up,down,left,right}. The key features include\nthe Manhattan distance from the evader to the three possible\ngoals (exits),{d(xe\u2212g1),d(xe\u2212g2),d(xe\u2212g3)}, and the\nManhattan distance between the agents, d(xe\u2212xo)and aAlgorithm 1: SFminmax: Turn-based Minmax Q\nlearning with SF and GGPI\ninput : discount factor \u00b5, task index k,\nlearning rate \u00b3\noutput : Successor features \u00c8\u03c0k\ni\n1InitializeQ0\u2208R|S\u00d7A\u00d7B|\n2forns steps do\n3 forego agent do\n4 ifBernoulli(\u03f5) = 1 then\n5 a\u2190Uniform (A)\n6 else\n7 at,bt=\nargmaxbargminamini\u03a8\u03a0i(s,a,b)Twi\n8 end\n9 Execute actions atand observe feature and\nreward\u03d5t,rtand new state s\u2032\nt\n10 ifnew task then\n11 a\u2032b\u2032= argmaxbargmina\u03a8\u03c0n+1wn+1\n12 \u03a8\u03c0n+1\nt+1(st,at,bt) = \u03a8\u03c0n+1\nt(st,at,bt)+\n\u00b3/braceleftbig\u03d5t+\u03a8t(s\u2032,a\u2032,b\u2032)\u2212\u03a8t(st,at,bt)/bracerightbig\n13 ifs\u2032\ntis not terminal then\n14 forother agent do\n15 ifBernoulli(\u03f5) = 1 then\n16 a\u2190Uniform (A)\n17 else\n18 at,bt=\nargmaxbargminamini\u03a8\u03a0i(s\u2032\nt,a,b)Twi\n19 end\n20 Execute actions atand observe feature\nand reward \u03d5t,rtand new state s\u2032\nt+1\n21 ifnew task then\n22 a\u2032b\u2032=\nargmaxbargmina\u03a8\u03c0n+1wn+1\n23 \u03a8\u03c0n+1\nt+1(s\u2032\nt,at,bt) =\n\u03a8\u03c0n+1\nt(s\u2032\nt,at,bt)+\n\u00b3/braceleftbig\u03d5t+\u03a8t(s\u2032\nt1,a\u2032,b\u2032)\u2212\u03a8t(st,at,bt)/bracerightbig\n24 end\n25 end\n26end\nterminal reward that the evader receives upon reaching the\nspeci\ufb01c goal,{rt(g1),rt(g2),rt(g3)}.\nTo normalize the reward, we divide the two distance-\nbased features dby the maximum possible distances between\nagents and the distance of evader from the goal. If the evader\nsuccessfully reaches the goal, it earns a token with a value of\n0.7, which is chosen based on parameter tuning. The feature\nweights determine the reward associated with each feature,\nand the weights are reset each time a new task begins.\nAn example of the feature weights used for training and\ntesting the environment is: [0.7,\u22121.3,0.7,0,0,0,0]. The\ncorresponding feature vector is de\ufb01ned as: \u03d5= [d(xe\u2212\nxo),d(xe\u2212g1),rt(g1),d(xe\u2212g2),rt(g2),d(xe\u2212g3),rt(g3)].\nFrom the ego agent\u2019s perspective, it receives a positive\nreward for maintaining a greater distance from other agents,\n\n--- Page 5 ---\nTABLE I\nTABLE OF TASKS AND THE WEIGHTS ASSOCIATED WITH THE TASKS\nMi weight (wi)\nTask 1 [0.7,-1.3, 0.7, 0, 0, 0, 0 ]\nTask 2 [0.7, 0, 0,-1.3, 0.7, 0, 0 ]\nTask 3 [0.7, 0, 0, 0, 0,-1.3, 0.7]\nincurs a high loss when far from its goal, and obtains a\nterminal reward upon reaching its goal. The reward for Task\n1 is computed by multiplying the feature vector, \u03d5, with the\nweight vector, w1following (1). The weights for Tasks 2\nand 3 are provided in Table I.\nWe run each game (task) for a designated number of\niterations or episodes. In the baseline scenario, the agents\nreset at the start of each new task. However, in our approach,\nthe agents carry over the SF table from the previous run.\nInitially, both agents are placed at equal distances from the\ngoal. For a more quantitative analysis, the agents\u2019 starting\npositions are randomized after each episode, i.e., when the\nevader reaches the goal or the pursuer catches the evader.\nFurther details of the experiment is explained in Sec. V-D.\n2)Compared Method and Evaluation Metrics :The per-\nformance of the ego agent is measured by the cumulative\nreward during training, with MinMaxQ-learning [27] as\nthe baseline. We further assess the policy generated across\ndifferent tasks, where both (ego and other) agents use the\nsame learning algorithm.\nAdditionally, we compare our method to other transfer\nlearning algorithms, including the Probabilistic Policy Reuse\n(PRQL) algorithm, which facilitates policy transfer between\ntasks [28]. Here, PRQL has been adapted for multi-agent\nscenarios. Furthermore, we carry an ablation test with the\n\u03f5reset version of SFminmaxQL as SF-r where at the be-\nginning of each task we reset \u03f5from Algorithm 1 line 4 to\npromote more exploration. For the sake of brevity, we will\nrefer to MinMaxQ-learning as \u201cMinmax\u201d and the original\nSFminmaxQL as \u201cSFminmax\u201d.\nThe aformentioned algorithms are further analyzed for\nrobustness under different initial conditions. We evaluate the\ndiscrepancy in values, denoted as |V\u2212V\u2032|, across all possible\ninitial positions for various tasks and episode lengths. Here,\nV\u2032refers to the value at 2,000 episodes for the minmax\nalgorithm, with a lower value indicating better performance.\nWe also report the success rate (SR), de\ufb01ned as the\npercentage of wins (pwin/ewin) and ties (tie) in each game.\nAn episode is considered successful if the pursuer catches\nthe evader or if the evader escapes within a 30-step limit.\nOtherwise, the episode is recorded as a tie. In addition,\nwe report Success Weighted by Path Length (SPL) [29],\nwhich accounts for the ef\ufb01ciency of the navigation process.\nSPL=1\nN/summationtextN\niSili\nmax(pi,li)wherepiis the path taken by the\nbaseline agent,000 episodes), liis the path taken by trained\nagent at given episode, Siis the binary indicator of success\nfor episode i, andNis the total number of episodes. The\nbaseline algorithm will reach a value of 1 by the end of the\ntraining while for the other trained agents, higher SPL scores\nindicates shorter path.\nFig. 1. Cumulative return per task in the Pursuer-Evader game. SFminmax\nreward uses the feature from the previous task at the beginning to each task.\nB. Reward Transfer: SFminmax vs. Baseline\nExperimental setup. The baseline used is the standard\nMinmax algorithm. Using the evaluation metrics discussed\nin Sec. V-A.2, we compare the baseline with the proposed\nSFminmax algorithm. In the \ufb01rst case study, both agents start\nat equal distances from the goal. The goal position remains\n\ufb01xed for the \ufb01rst 30,000 iterations, after which it changes. At\nthis point, the baseline algorithms reset, while the proposed\nSFminmax algorithm continues using the SF table. We adopt\na discount factor of \u00b5= 0.9and a learning rate of \u00b3= 0.5.\nIn this experiment, we focus on the cumulative reward during\nthe training phase, where the reward represents the ego\nagent\u2019s reward minus that of the other agent.\nObservation. Our results demonstrate that SFminmax suc-\ncessfully transfers knowledge from one task to another,\nresulting in higher cumulative return for the reward during\ntraining compared to the baseline algorithm as seen in Fig.\n1.\nC. Test Policy Transfer: SFminmax vs. Baseline\nExperimental setup. The experimental setup mirrors the one\nused in the previous case study. However, instead of evaluat-\ning the value function, we focus on the agents\u2019 policies. As\noutlined in the previous section, testing is conducted every\n10,000 iterations and at the start of each new task. Here, we\nexamine the agents\u2019 policies at speci\ufb01c iterations to assess\nwhether the transfer of knowledge is successful.\nObservation. SFminmax can transfer knowledge across\ntasks within the policy space, as seen in Fig. 2. The baseline\nMinmax algorithm struggles to converge to a model when\nthe number of iterations is limited. In contrast, the proposed\nalgorithm provides a jump-start for new tasks, enabling the\ngeneration of converging policies. When transferring from\nTask 1 to Task 2, we observe near-instantaneous results, with\npolicies converging after 0 iterations as seen in Fig. 2(b).\nFor Task 3, however, due to the different end-goal positions,\npolicy convergence occurs within 10,000 iterations as seen\nin Fig. 2(c), whereas the baseline algorithm fails to achieve\nsimilar performance within 10,000 iterations as shown in\nFig. 2(d).\n\n--- Page 6 ---\nFig. 2. Trajectories taken by the evader (robber) and pursuer (police) for three tasks determined by the goal position (exits). The trajectories for case\nstudy (a) at the end of 30,000 iterations have the evader reach the goal. (b) shows the one-shot transfer of the policy when the goal position changes. (c)\nThe trajectory takes 10000 more interaction to converge in Task 2. (d) The baseline does not converge with the same number of episodes.\nTABLE II\nNAVIGATION PERFORMANCE COMPARISON IN DIFFERENT TEST TASKS\n|V\u2212V\u2032| SR % [ewin, tie] SPL\nEps Minmax SFminmax SF-r PRQL Minmax SFminmax Sf-r PRQL Minmax SFminmax Sf-r PRQL\nTask 2\n0 0.535 0.07 0.345 1.03 [55.5, 33.3] [100, 0] [66.7, 33.3] [22.2, 33.3] 0.46 1.06 0.70 0.2\n1000 0.01 0.03 0.452 0.11 [100, 0] [100, 0] [44.4,55.6] [77.8, 0.0] 0.80 1.06 0.43 0.89\n2000 NA 0.128 0.345 0.07 [88.8, 0] [88.8, 0.0] [55.5, 44.4] [77.8, 0.0] 1.0 1.09 0.62 0.83\nTask 3\n0 0.900 0.141 0.331 0.612 [22.2. 11.1] [88.9, 0.0] [88.8, 0.0] [44.44, 22] 0.43 1.29 1.15 0.26\n1000 0.176 0.193 0.210 0.15 [100, 0] [100, 0.0] [100.0, 0] [77.78, 0.0] 1.0 1.15 0.74 1.15\n2000 NA 0.458 0.158 0.17 [88.8, 0] [77.8, 11.1] [88.8, 0.0] [55.6, 0.0] 1.0 1.05 0.97 1.05\nTask 4\n0 0.87 0.621 0.355 1.307 [33.3, 44.4] [44.4, 0.0] [88.8, 0.0] [11.1, 22.2] 0.24 1.39 1.68 0.5\n1000 0.128 0.319 0.505 0.197 [100, 0] [66.7. 11.1] [66.6, 0.0] [55.6, 0] 0.74 1.19 1.09 0.91\n2000 NA 0.448 0.44 0.180 [88.8, 0] [ 66.7, 0.0 ] [ 66.6, 0.0 ] [77.8, 0] 1.0 1.09 1.17 1.12\nD. Transfer with Signi\ufb01cant Changes in Reward\nExperimental setup. We \ufb01rst pre-train the model where the\ninitial positions, xeandxo, are randomly sampled from xe\u2208\n[1,2,3]for thex-coordinate, where oandestand for the\npursuer/other and evader/ego in the game, respectively. The\ny-coordinate \ufb01xed at 0. This pre-training is done on Task 1,\nwhere the goal position is set to (2,5) as shown in Fig. 3(a).\nAfter pre-training, we retrain the model on new tasks with\nchanging goal positions: Task 2 has a goal at (2, 4), Task 3 at\n(2, 1), and Task 4 at (2, 0). SFs for different initial and goal\nconditions for single agent have been previously studied in\n[30]. In this work, we explore how SFs with GGPI facilitates\ntransfer to new goal positions in multi agent games. During\ntraining, we evaluate the performance at the midpoint of the\ngame and examine the results after \ufb01nal convergence.\nHyperparameter tuning. We use a decaying \u03f5for each\nnew task, following the method proposed in [30], for all\nalgorithms except for SFminmax. We conducted an ablation\nstudy with learning rates \u00b3\u2208[0.1,0.2,0.3,0.4,0.5]from\nAlgorithm 1 line 12 and baseline algorithms to identify\nthe signi\ufb01cance of priority of new to old experience in all\ntemporal difference algorithms. We found the best results\nfor the baseline algorithm with \u00b3minmax= 0.3and for our\nalgorithm with \u00b3sf= 0.1.\nObservation. First, we analyze the average reward of the ego\nagents, averaged over nine initial conditions. The rewards aresampled \ufb01ve times, every 500 episodes from 0-2000 episode\nper task. We observe that the normalized reward values for\nagents using SFs are higher compared to those without SF,\nas shown in Fig. 3(b). To further investigate the optimality\nof the solution, we also summarize the results in Table II.\nThe key observations are as follows.\nOne-shot transfer : The one-shot transfer of both SF-based\nalgorithms (SFminmax and SF-r) perform better compared\nto random restart from the other algorithms as seen by lower\n|V\u2212V\u2032|at timestep 0 for SF algorithms.\nSuccess rates : As episodes increase (e.g., from 0 to 2,000),\nsome algorithms show variations in SR%. The reduction\non the tie in SR% indicates learning for both Minmax and\nPRQL algorithm. However, we observe the same win rate\nin all the three tasks for Minmax, indicating bias. Here, the\nevader agent wins in eight out of nine scenarios, even in\nscenarios where the pursuer has advantage. This can be seen\nin Fig. 3(a). At t= 0, the pursuer is closer to goal and has\nadvantage. In Fig. 3(c) for Minmax agents, the pursuer is not\nable to intervene and evader agent reaches the goal at t= 14 .\nHowever, the pursuer agent is able to catch the evader agent\nand \ufb01nish the game within t= 6, as shown in Fig. 3(b).\nEf\ufb01ciency of Policy : Among the successful tasks we see\nthat SFminmax consistently shows higher SPL values across\ntasks compared to Minmax, SF-r and PRQL. This shows\nthat transfer in value and policy lead to shorter path length.\n\n--- Page 7 ---\nFig. 3. (a) Possible initial position and goals for testing transfer with signi\ufb01cant reward change (b) Path taken by SFminmax agents where pursuer agent\nis able to intervene. (c) Path taken my Minmax agents where pursuer is not to intervene (d) Cumulative return per task in the Pursuer-Evader game.\nSFminmax reward uses the feature from the previous task at the beginning to each task.\nImpact of Task Variability : As the task keeps changing dur-\ning training, a signi\ufb01cant deviation from the original task\nleads to performance degradation in SFminmax, particularly\nwhen comparing|V\u2212V\u2032|from Task 2 to Task 4 at 2000\nepisodes.SF-r, however, outperforms the original SFminmax\nas observed in value discrepancy for Task 3. Additionally,\nPRQL algorithms tend to perform better with more training\nepisodes and when the test task differs more signi\ufb01cantly\nfrom the trained task. This is evident in Task 4, where PRQL\nachieves lower|V\u2212V\u2032|as well as higher SPL scores. The\nvalue discrepancy arises party differences in agent wins,\nas re\ufb02ected in SR%. For instance, Minmax achieves an\newin/pwin rate of 88.9%/11.1% in Task 4, whereas SF\nalgorithms result in a 66.7%/33.3% split.\nVI. C ONCLUSION , LIMITATION AND DISCUSSION\nIn this paper, we introduced an algorithm that leverages\nSFs with GGPI to enable transfer in alternating Markov\ngames. We analyzed the convergence properties and establish\na transfer bound for the novel GGPI algorithm, extending\nthe GPI for games. We evaluated the proposed algorithm in\na grid-based pursuer-evader game, demonstrating successful\npolicy and value transfer for cases with the same initial\npositions. To test transfer with signi\ufb01cant change in reward,\nwe further carried out an experiment with different initial\nand goal positions for our proposed algorithm and other\nbaseline algorithms. Here, we showed that our proposed\nalgorithm performs better in terms of initial value transfer,\nshorten the time taken to complete the game in successes,\nand generate higher normalized reward for all the tasks and\ninitial positions. Finally, we highlighted the advantages of\nexploration when the test task signi\ufb01cantly differs from the\ntraining task by evaluating a reset version of our transfer\nalgorithm (SF-r) alongside PRQL.\nNotably, the SFminmax and SF-r approaches, while show-\ning signi\ufb01cant promise, have certain limitations. The use\nof the GGPI algorithm requires a discrete action space\nand assumes a conservative strategy by modeling the op-\nponent\u2019s best response. Additionally, it relies on the Q-\nvalues of the training and test tasks being within comparablebounds. While this approach works well when tasks have\nsimilar reward structures or importance, it becomes less\neffective when the nature or weighting of the tasks changes\nsigni\ufb01cantly. Consequently, the proposed algorithm can be\nextended to a continuous state environment where further\ntests can be carried out in similar zero-sum game tasks with\nconvergence guarantees [31], [32]. Similarly, SFs can be\nexplored in turn-based extensive-form games, where a new\npolicy improvement algorithm could be developed to both\nestablish transfer bounds and enhance successful transfer.\nACKNOWLEDGMENT\nThe authors would like to thank Mukesh Ghimire and Lei\nZhang for assistance with verifying the algorithm.\nREFERENCES\n[1] L. Canese, G. C. Cardarilli, L. Di Nunzio, R. Fazzolari, D. Giardino,\nM. Re, and S. Span `o, \u201cMulti-agent reinforcement learning: A review\nof challenges and applications,\u201d Applied Sciences , vol. 11, no. 11, p.\n4948, 2021.\n[2] B. Baker, I. Kanitscheider, T. Markov, Y . Wu, G. Powell, B. McGrew,\nand I. Mordatch, \u201cEmergent tool use from multi-agent autocurricula,\u201d\narXiv preprint arXiv:1909.07528 , 2019.\n[3] C. Ma, A. Li, Y . Du, H. Dong, and Y . Yang, \u201cEf\ufb01cient and scalable re-\ninforcement learning for large-scale network control,\u201d Nature Machine\nIntelligence , vol. 6, no. 9, pp. 1006\u20131020, 2024.\n[4] F. L. Da Silva and A. H. R. Costa, \u201cA survey on transfer learning\nfor multiagent reinforcement learning systems,\u201d Journal of Arti\ufb01cial\nIntelligence Research , vol. 64, pp. 645\u2013703, 2019.\n[5] S. Amatya, M. Ghimire, Y . Ren, Z. Xu, and W. Zhang, \u201cWhen shall\ni estimate your intent? costs and bene\ufb01ts of intent inference in multi-\nagent interactions,\u201d in 2022 American Control Conference (ACC) .\nIEEE, 2022, pp. 586\u2013592.\n[6] J. Sinapov, S. Narvekar, M. Leonetti, and P. Stone, \u201cLearning inter-task\ntransferability in the absence of target task samples,\u201d in Proceedings\nof the 2015 international conference on autonomous agents and\nmultiagent systems , 2015, pp. 725\u2013733.\n[7] A. Braylan and R. Miikkulainen, \u201cObject-model transfer in the general\nvideo game domain,\u201d in Proceedings of the AAAI Conference on\nArti\ufb01cial Intelligence and Interactive Digital Entertainment , vol. 12,\nno. 1, 2016, pp. 136\u2013142.\n[8] S. Barrett and P. Stone, \u201cCooperating with unknown teammates in\ncomplex domains: A robot soccer case study of ad hoc teamwork,\u201d in\nProceedings of the AAAI Conference on Arti\ufb01cial Intelligence , vol. 29,\nno. 1, 2015.\n[9] M. L. Littman et al. , \u201cFriend-or-foe q-learning in general-sum games,\u201d\ninProceedings of the International Conference on Machine Learning ,\nvol. 1, 2001, pp. 322\u2013328.\n\n--- Page 8 ---\n[10] A. Barreto, W. Dabney, R. Munos, J. J. Hunt, T. Schaul, H. P. van\nHasselt, and D. Silver, \u201cSuccessor features for transfer in reinforce-\nment learning,\u201d Advances in neural information processing systems ,\nvol. 30, 2017.\n[11] W. Liu, L. Dong, D. Niu, and C. Sun, \u201cEf\ufb01cient exploration for multi-\nagent reinforcement learning via transferable successor features,\u201d\nIEEE/CAA Journal of Automatica Sinica , vol. 9, no. 9, pp. 1673\u20131686,\n2022.\n[12] T. Hinrichs and K. D. Forbus, \u201cTransfer learning through analogy in\ngames,\u201d Ai Magazine , vol. 32, no. 1, pp. 70\u201370, 2011.\n[13] M. Sharma, M. P. Holmes, J. C. Santamar \u00b4\u0131a, A. Irani, C. L. Isbell Jr,\nand A. Ram, \u201cTransfer learning in real-time strategy games using\nhybrid cbr/rl.\u201d in IJCAI , vol. 7, 2007, pp. 1041\u20131046.\n[14] C. Cheng, Z. Zhu, B. Xin, and C. Chen, \u201cA multi-agent reinforcement\nlearning algorithm based on stackelberg game,\u201d in 2017 6th Data\nDriven Control and Learning Systems (DDCLS) . IEEE, 2017, pp.\n727\u2013732.\n[15] Y . Liu, Y . Hu, Y . Gao, Y . Chen, and C. Fan, \u201cValue function transfer\nfor deep multi-agent reinforcement learning based on n-step returns.\u201d\ninIJCAI . Macao, 2019, pp. 457\u2013463.\n[16] Y . Chen, L. Zhang, T. Merry, S. Amatya, W. Zhang, and Y . Ren,\n\u201cWhen shall i be empathetic? the utility of empathetic parameter\nestimation in multi-agent interactions,\u201d in 2021 IEEE International\nConference on Robotics and Automation (ICRA) . IEEE, 2021, pp.\n2761\u20132767.\n[17] Y . Hu, Y . Gao, and B. An, \u201cAccelerating multiagent reinforcement\nlearning by equilibrium transfer,\u201d IEEE transactions on cybernetics ,\nvol. 45, no. 7, pp. 1289\u20131302, 2014.\n[18] A. Barreto, S. Hou, D. Borsa, D. Silver, and D. Precup, \u201cFast\nreinforcement learning with generalized policy updates,\u201d Proceedings\nof the National Academy of Sciences , vol. 117, no. 48, pp. 30 079\u2013\n30 087, 2020.\n[19] M. Gimelfarb, A. Barreto, S. Sanner, and C.-G. Lee, \u201cRisk-aware\ntransfer in reinforcement learning using successor features,\u201d Advances\nin Neural Information Processing Systems , vol. 34, pp. 17 298\u201317 310,\n2021.\n[20] M. Abdolshah, H. Le, T. K. George, S. Gupta, S. Rana, and\nS. Venkatesh, \u201cA new representation of successor features for trans-\nfer across dissimilar environments,\u201d in International Conference on\nMachine Learning . PMLR, 2021, pp. 1\u20139.\n[21] C. Szepesv \u00b4ari and M. L. Littman, \u201cGeneralized markov decision\nprocesses: Dynamic-programming and reinforcement-learning algo-\nrithms,\u201d in Proceedings of International Conference of Machine Learn-\ning, vol. 96, 1996.\n[22] M. L. Littman, \u201cMarkov games as a framework for multi-agent rein-\nforcement learning,\u201d in Machine learning proceedings 1994 . Elsevier,\n1994, pp. 157\u2013163.\n[23] D. Bertsekas, Dynamic programming and optimal control: Volume I .\nAthena scienti\ufb01c, 2012, vol. 4.\n[24] Y . Zhu and D. Zhao, \u201cOnline minimax q network learning for\ntwo-player zero-sum markov games,\u201d IEEE Transactions on Neural\nNetworks and Learning Systems , vol. 33, no. 3, pp. 1228\u20131241, 2020.\n[25] R. A. Howard, \u201cDynamic programming and markov processes.\u201d 1960.\n[26] D. Bertsekas and J. N. Tsitsiklis, Neuro-dynamic programming .\nAthena Scienti\ufb01c, 1996.\n[27] D. Lee, \u201cFinite-time analysis of minimax q-learning for two-player\nzero-sum markov games: Switching system approach,\u201d arXiv preprint\narXiv:2306.05700 , 2023.\n[28] F. Fern \u00b4andez, J. Garc \u00b4\u0131a, and M. Veloso, \u201cProbabilistic policy reuse\nfor inter-task transfer learning,\u201d Robotics and Autonomous Systems ,\nvol. 58, no. 7, pp. 866\u2013871, 2010.\n[29] P. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta,\nV . Koltun, J. Kosecka, J. Malik, R. Mottaghi, M. Savva et al. ,\n\u201cOn evaluation of embodied navigation agents,\u201d arXiv preprint\narXiv:1807.06757 , 2018.\n[30] L. Lehnert, S. Tellex, and M. L. Littman, \u201cAdvantages and limitations\nof using successor features for transfer in reinforcement learning,\u201d\narXiv preprint arXiv:1708.00102 , 2017.\n[31] R. Lowe, Y . I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mor-\ndatch, \u201cMulti-agent actor-critic for mixed cooperative-competitive\nenvironments,\u201d Advances in neural information processing systems ,\nvol. 30, 2017.\n[32] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and\nS. Levine, \u201cMeta-world: A benchmark and evaluation for multi-taskand meta reinforcement learning,\u201d in Conference on robot learning .\nPMLR, 2020, pp. 1094\u20131100.\n\n--- Page 9 ---\n1\nI. P ROOF OF THEORETICAL RESULTS\nA. Proof of Proposition I\nTheorem 1. (Game Generalized Policy Improvement) Let\n\u03a01,\u03a02,...\u03a0nbendecision policies and let \u02dcQ\u03a01,\u02dcQ\u03a02,...\u02dcQ\u03a0n\nbe approximates of their respective action value function such\nthat\n|Q\u03a0i(s,a,b)\u2212\u02dcQ\u03a0i(s,a,b)| \u2264\u03f5\nfor alls\u2208S,a\u2208A,b\u2208Bandi\u2208 {1,2,...n}.\nDe\ufb01ne\n\u00c3(s)\u2208argmax\namin\nbmin\ni\u02dcQ\u03a0i(s,a,b)\nWe start with the assumptions necessary for this learning\nalgorithm to satisfy the conditions of Theorem 1 in [1]\nand therefore converge to optimal Q values. The dynamic\nprogramming operator de\ufb01ning the optimal Q function is.\nProof. Simplifying the notation, let\nQmin(s,a,b) =miniQ\u03a0i(s,a,b)and\n\u02dcQmin(s,a,b) = min\ni\u02dcQ\u03a0i(s,a,b)\nWe start by noting that for any s\u2208Sand anya\u2208Aand any\nb\u2208Bthe following holds:\n|Qmin(s,a,b)\u2212\u02dcQmin(s,a,b)|=|min\niQ\u03a0i(s,a,b)\u2212\nmin\ni\u02dcQ\u03a0i(s,a,b)|= min\ni|Q\u03a0i(s,a,b)\u2212\u02dcQ\u03a0i(s,a,b)| \u2264\u03f5\nThis property should remain at a minimum as well as a\nmaximum.\nFor alls\u2208Sanda\u2208Aandi\u22081,2, we have\nT\u03a0\u02dcQmin(s,a,b) =r(s,a,b)+/summationdisplay\nsp(s\u2032|s,a,b)\u02dcQmin(s\u2032,\u03a0(s\u2032))\n=r(s,a,b)+/summationdisplay\nsp(s\u2032|s,a,b)max\namin\nb\u02dcQmin(s\u2032,a,b)\n\u2265r(s,a,b)+/summationdisplay\nsp(s\u2032|s,a,b)max\namin\nbQmin(s\u2032,a,b)\u2212\u00b5\u03f5\nthis is property of Bellman Operator\n\u2265r(s,a,b)+/summationdisplay\nsp(s\u2032|s,a,b)Qmin(s\u2032,\u03a0i(s\u2032))\u2212\u00b5\u03f5\n\u2265r(s,a,b)+/summationdisplay\nsp(s\u2032|s,a,b)Q\u03a0i(s\u2032,\u00c3i(s\u2032))\u2212\u00b5\u03f5\n=T\u03a0iQ\u03a0i(s,a,b)\u2212\u00b5\u03f5\n=Q\u03a0i(s,a,b)\u2212\u00b5\u03f5\nSinceT\u03a0\u02dcQmin(s,a,b)\u2265Q\u03a0\ni(s,a,b)\u2212\u00b5\u03f5for anyitask, it\nmust be the case that\nT\u03a0\u02dcQmin(s,a,b)\u2265min\niQ\u03a0i(s,a)\u2212\u00b5\u03f5\n=Qmin(s,a)\u2212\u00b5\u03f5\n\u2265\u02dcQmin\u2212\u03f5\u2212\u00b5\u03f5\nThe Bellman operator in reinforcement learning is said to\nhave two key properties: monotonicity and contraction.\nMonotonicity :De\ufb01nition: A mapping Tis said to be monotonic if, for\nany two functions V1andV2such thatV1\u2264V2, pointwise, it\nfollows that TV1\u2264TV2pointwise.\nIn the context of the Bellman operator: If Q1\u2264Q2pointwise\n(meaningQ1(s,a)\u2264Q2(s,a)for all s and a), then it implies\nthatTQ1\u2264TQ2. In other words, improving the estimate of\nthe Q-values for state-action pairs will result in an improved\nestimate after applying the Bellman operator.\nContraction (or contraction mapping) property :\nDe\ufb01nition: A mapping Tis a contraction if there exists a\nconstant0\u2264\u00b5 <1such that, for all functions V1andV2, it\nfollows that ||TV1\u2212TV2|| \u2264\u00b5||V1\u2212V2||where||.||denotes\nsome norm.\nIn the context of the Bellman operator: If Tis a contraction,\napplying the Bellman operator to two different Q-value func-\ntions results in Q-value functions that are closer together. This\nproperty is particularly useful in iterative algorithms because\nit guarantees convergence to a unique \ufb01xed point.\nIn summary, the monotonicity property ensures that im-\nprovements in the Q-value estimates lead to improvements af-\nter applying the Bellman operator, and the contraction property\nguarantees the convergence of iterative methods to a unique\nsolution.\nThese properties are crucial in the analysis of reinforcement\nlearning algorithms, especially those based on iterative meth-\nods like value iteration or Q-learning. They provide theoretical\nguarantees on the convergence of the algorithms and the\nconsistency of the estimated values.\nNow we look into the \ufb01xed point theorem: Simplifying the\nBellman operator under the assumptions of a deterministic\npolicy and a constant function e(s,a) = 1 for all s,a.\nStarting with the Bellman operator:\nT\u03c0Q(s,a) =/summationdisplay\ns\u2032P(s\u2032|s,a)[R(s,a,s\u2032)+\u00b5/summationdisplay\na\u2032\u00c3(a\u2032|s\u2032)Q(s\u2032,a\u2032)].\n(1)\nAssumptions: \u00c3(a\u2032|s\u2032) = 1 (deterministic policy)\ne(s,a) = 1 for all s,a\nNow, let\u2019s apply these assumptions to simplify the Bellman\noperator:\nT\u03c0(\u02dcQmin(s,a)+ce(s,a)) =/summationdisplay\ns\u2032P(s\u2032|s,a)[R(s,a,s\u2032)+\n\u00b5/summationdisplay\na\u2032\u00c3(a\u2032|s\u2032)(\u02dcQmin(s\u2032,a\u2032)+ce)].\nGiven the deterministic policy, \u00c3(a\u2032|s\u2032) = 1 , so the summation\nover a\u2019 simpli\ufb01es:\n=/summationdisplay\ns\u2032P(s\u2032|s,a)[R(s,a,s\u2032)+\u00b5(\u02dcQmin(s\u2032,a\u2032)+ce)].(2)\nNow, sincee(s,a) = 1 for all s,a, the term ce(s,a)becomes\nc, and we have:\n=/summationdisplay\ns\u2032P(s\u2032|s,a)[R(s,a,s\u2032)+\u00b5(\u02dcQmin(s\u2032,a\u2032)+c)].(3)\nExpanding the sum over s\u2019 and using the fact that P(s\u2032|s,a)\nis a probability distribution and considering the fact that\n\n--- Page 10 ---\n2\n\u00c3(a\u2032|s\u2032) = 1 implies that there is only one relevant a\u2019 for\neach s\u2019, we can simplify further:\n=R(s,a,s\u2032)+\u00b5(\u02dcQmin(s\u2032,a\u2032,)+c)] (4)\nFinally, the expression becomes:\n=\u02dcQmin(s,a)+\u00b5c (5)\nNow, we look into how the property can be used for multi-\nagent settings, if the policy is deterministic the above property\nholds:\nNow we want to prove that:\nFirst and foremost:\nQ\u03a0(s,a,b) = lim\nk\u2192\u221e(T\u03c0)k\u02dcQmin(s,a,b)\n=limk+1\u2192\u221e(T\u03c0)k+1T\u03c0\u02dcQmin(s,a,b)\n\u2265limk+1\u2192\u221e(T\u03c0)k(\u02dcQmin(s,a,b)\u2212\u03f5(1+\u00b5))\n=limk+2\u2192\u221e(T\u03c0)kT\u03c0(\u02dcQmin(s,a,b)\u2212\u03f5(1+\u00b5))\n=limk+2\u2192\u221e(T\u03c0)k(\u02dcQmin(s,a,b)\u2212\u00b5\u03f5(1+\u00b5))\n\u2265\u02dcQmin\u2212\u03f51+\u00b5\n1\u2212\u00b5: geometric expansion if \u00b5\u22641\n=Qmin(s,a,b)\u2212\u03f5\u2212\u03f51+\u00b5\n1\u2212\u00b5\nProof: the result is a direct application of the theorem 1 and\nLemma 1. For any j\nB. Proof of Lemma 1\nLemma 1. : Let\u00b6ij= max s,a,b|ri(s,a,b)\u2212rj(s,a,b)|and\nlet\u03a0be an arbitrary policy. Then,\n|Q\u03a0\ni(s,a,b)\u2212Q\u03a0\nj(s,a,b)| \u2264\u00b6ij\n1\u2212\u00b5\nProof. Let us simplify the notation, now let Qj\ni(s,a,b) =\nQ\u03a0\u2217\nj\ni(s,a,b).Then,\nQi\ni(s,a,b)\u2212Qj\ni(s,a,b) =Qi\ni(s,a,b)\u2212Qj\nj(s,a,b)+\nQj\nj(s,a,b)\u2212Qj\ni(s,a,b)\u2264\n|Qi\ni(s,a,b)\u2212Qj\nj(s,a,b)|+|Qj\nj(s,a,b)\u2212Qj\ni(s,a,b)|\nOur strategy will be to bound |Qi\ni(s,a,b)\u2212Qj\nj(s,a,b)|and\n|Qj\nj(s,a,b)\u2212Qj\ni(s,a,b)|Note that |Qi\ni(s,a,b)\u2212Qj\nj(s,a,b)|\nis the difference between the value functions of two Markov\nGames with the same transition function but potentially dif-\nferent rewards.De\ufb01ne\u2206ij= max s,a,b|Qi\ni(s,a,b)\u2212Qj\nj(s,a,b)|. Then,\n|Qi\ni(s,a,b)\u2212Qj\nj(s,a,b)|\n=|ri(s,a,b)+/summationdisplay\ns\u2032p(s\u2032|s,a,b)max\na\u2032\u2208Amin\nb\u2032\u2208BQ\u03a0\ni(s\u2032,a\u2032,b\u2032)\n\u2212rj(s,a,b)\u2212/summationdisplay\ns\u2032p(s\u2032|s,a,b)max\na\u2032\u2208Amin\nb\u2032\u2208BQ\u03a0\nj(s\u2032,a\u2032,b\u2032)|\n=|ri(s,a,b)\u2212rj(s,a,b)+/summationdisplay\ns\u2032p(s\u2032|s,a,b)/parenleftbig\nmax\na\u2032\u2208Amin\nb\u2032\u2208BQ\u03a0\ni(s\u2032,a\u2032,b\u2032)\n\u2212max\na\u2032\u2208Amin\nb\u2032\u2208BQ\u03a0\nj(s\u2032,a\u2032,b\u2032)/parenrightbig\n|\n\u2264 |ri(s,a,b)\u2212rj(s,a,b)|+/summationdisplay\ns\u2032p(s\u2032|s,a,b)|max\na\u2032\u2208Amin\nb\u2032\u2208BQ\u03a0\ni(s\u2032,a\u2032,b\u2032)\n\u2212max\na\u2032\u2208Amin\nb\u2032\u2208BQ\u03a0\nj(s\u2032,a\u2032,b\u2032)|\n\u2264\u00b6ij+\u2206ij.\nWe now turn our attention to |Qj\nj(s,a,b)\u2212Qj\ni(s,a,b)|. Fol-\nlowing the previous step: de\ufb01ne \u2206\u2032\nij= max s,a,b|Qj\nj(s,a,b)\u2212\nQj\ni(s,a,b)|. Then,\n|Qj\nj(s,a,b)\u2212Qj\ni(s,a,b)|\n=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsinglerj(s,a,b)+\u00b5/summationdisplay\ns\u2032p(s\u2032|s,a,b)Qj\nj(s\u2032,\u03a0\u2217\nj(s\u2032))\n\u2212ri(s,a,b)\u2212\u00b5/summationdisplay\ns\u2032p(s\u2032|s,a,b)Qj\ni(s\u2032,\u03a0\u2217\nj(s\u2032))/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle\n=|ri(s,a,b)\u2212rj(s,a,b)\n+\u00b5/summationdisplay\ns\u2032p(s\u2032|s,a,b)/parenleftBig\nQj\nj(s\u2032,\u03a0\u2217\nj(s\u2032))\u2212Qj\ni(s\u2032,\u03a0\u2217\nj(s\u2032))/parenrightBig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle\n\u2264 |ri(s,a,b)\u2212rj(s,a,b)|\n+\u00b5/summationdisplay\ns\u2032p(s\u2032|s,a,b)|Qj\nj(s\u2032,\u03a0\u2217\nj(s\u2032))\u2212Qj\ni(s\u2032,\u03a0\u2217\nj(s\u2032))|\n\u2264\u00b6ij+\u2206\u2032\nij.\nSolving for the \u2206\u2032\nij, you get\n\u2206\u2032\nij\u2264\u00b6ij\n1\u2212\u00b5(6)\nNow for the desired result:\nQi\ni(s,a,b)\u2212Qj\ni(s,a,b)\u2264 |Qi\ni(s,a,b)\u2212Qj\nj(s,a,b)|+\n|Qj\nj(s,a,b)\u2212Qj\ni(s,a,b)| \u22642\u00b6ij\n1\u2212\u00b5\nProposition 1 . LetMi\u2208 M\u03c6and letQ\u03a0\u2217\nj\nibe the value\nfunction of an optimal policy of Mj\u2208 M\u03c6when executed in\nMi. Given the set {\u02dcQ\u03a0\u2217\n1\ni,\u02dcQ\u03a0\u2217\n2\ni,...\u02dcQ\u03a0\u2217\nn\ni}such that\n|Q\u03a0\u2217\nj\ni(s,a,b)\u2212\u02dcQ\u03a0\u2217\nj\ni(s,a,b)| \u2264\u03f5for alls\u2208S,\na\u2208Aandj\u22081,2,...n,\nlet,\n\u00c3(s)\u2208argmax\namin\nbmin\nj\u02dcQ\u03a0j\ni(s,a,b)\n\n--- Page 11 ---\n3\nThen,\nQ\u2217\ni(s,a,b)\u2212Q\u03a0\ni(s,a)\n\u22642\n1\u2212\u00b5max\ns,a,b|ri(s,a,b)\u2212rj(s,a,b)|+2\n1\u2212\u00b5\u03f5\nProof, The result is a direct application of Theorem 1 and\nLemmas 1 and 2. For any j\u2208 {1,2,...n}, we have.\nQ\u2217\ni(s,a,b)\u2212Q\u03a0\ni(s,a)\n\u2264Q\u2217\ni(s,a,b)\u2212Qj\ni(s,a)+2\n1\u2212\u00b5\u03f5Theorem 1\n\u22642\n1\u2212\u00b5\u00b6ij+2\n1\u2212\u00b5\u03f5Lemma 1\n=2\n1\u2212\u00b5max\ns,a,b|ri(s,a,b)\u2212rj(s,a,b)|+2\n1\u2212\u00b5\u03f5\nII. I MPLEMENTATION DETAILS\nIn this section we describe in detail of the environmental\nsetup and training details of our empirical studies. Pursuer\nEvader is a standard experiment for zero sum game, we\nadopted the hyperparameters used in the [2]. We also intro-\nduced a challenging task in the Pursuer Evader game which\nhas different initial conditions and more possible goals.\nA. Pursuer-Evader for Reward and Policy Transfer\nSection 5 of the paper provides an intuitive overview of\nthe pursuer-evader game used in our experiments. In our\nsetup, when transitioning to a new task, the successor features\nare initialized using those learned from the previous task.\nSpeci\ufb01cally, at the start of Task 2, the successor feature table\nfrom Task 1 is copied, and the policy is initialized using the\nGGPI algorithm. We use a discount factor of \u00bc= 0.9, which\nis a standard choice and yielded the best results in our tests.\nFig. 1. All the initial position and goal for the quantitative test.1) Preliminary Tests: Two key features in the pursuer-\nevader tasks are: (1) the distance between the agents, d(xe\u2212\nxo), and (2) the distance between the evader and potential\ngoal positions, d(xe\u2212gn), wherendenotes the set of possible\ngoals/tasks. In our preliminary experiments, we conducted task\ntransfer from Task 1 to Task 2 using 20,000 training iterations\non Task 1, without incorporating terminal rewards. However,\nthis setup did not lead to improvements in cumulative returns\nor effective policy transfer.\nTo address this, we introduced a terminal reward feature,\nrt(gn), and additionally incorporated feature weights wto\nbalance the in\ufb02uence of the two distance-based features. An\nablation study revealed that the weights listed in Table 1\nyielded the best results in terms of both reward transfer and\none-shot policy transfer in Task 2.\nB. Pursuer Evader Qualitative Test\nIn this section we provide more information on the pursuer\nevader game and possible output for both agents with different\ninitial positions. As seen in Fig: 1, there are total of 9\ncombination of initial position for the agents to move towards\nthe goal and the the exit number infront of each door is the\ntasks.\nBecause of the increase in the number of the doors, the\nnew task is described by weights, the length of weights differs\nfrom 8 to 10. Hence , like in Case study 1, the task is given\nby.[0.7,\u22121.3,0.7,0,0,0,0,0,0]where the \ufb01rst weight is for\nmanhattan distance between the agents, the next two parameter\nfor the Task 1 ( the weight for distance from evader to goal\nand the weight of terminal reward for evader), after that the\nsubsequent two are for the next task in the increasing order.\nC. Algorithms\nAs mentioned in the Section 4 of the paper, we have both\nagents updating their TD error at the same time. Figure 2\nshows a time scale of how both agents use a one step horizon\nlook out for the other agent\u2019s action and choose to maximize\nbased on the current reward.\nFig. 2. Asynchronous q table update with one step look out horizon for both\nagents.\nThis has been implemented for all the algorithms where\nboth agents are able to have a one step look ahead into the\nopponent\u2019s policy.\n\n--- Page 12 ---\n4\nFig. 3. Schematic representation of how the \u03c8is taken from the table and GGPI is implemented. Given a state s\u2208S, the \ufb01rst step is to compute,for each\na\u2217b\u2208A\u00d7B, the values of the n SFs: \u03c8\u03c0i(s,a,b). We have sf-table for each task for both pursuer and evader. Based on the agent who is taking the turn, we\nchoose the table. After this, we calculate n Q values by the multiplication of the SFs with Task Preferences w, the result of which is an n\u00d7|A|\u00d7|B|matrix\nwhose(i,a,b)entry isQ\u03c0i(s,a,b). GPI then consists in \ufb01nding the minimum element of this matrix and returning the associated actions aandb.\nFig. 3 provides a schematic view of the computation of \u00c8\nand the use GGPI to \ufb01nd actions at a timestep.\nREFERENCES\n[1] M. L. Littman and C. Szepesv \u00b4ari, \u201cA generalized reinforcement-learning\nmodel: Convergence and applications,\u201d in ICML , vol. 96, 1996, pp. 310\u2013\n318.\n[2] A. Barreto, W. Dabney, R. Munos, J. J. Hunt, T. Schaul, H. P. van Hasselt,\nand D. Silver, \u201cSuccessor features for transfer in reinforcement learning,\u201d\nAdvances in neural information processing systems , vol. 30, 2017.",
  "project_dir": "artifacts/projects/enhanced_cs.MA_2507.22278v1_Successor_Features_for_Transfer_in_Alternating_Mar",
  "communication_dir": "artifacts/projects/enhanced_cs.MA_2507.22278v1_Successor_Features_for_Transfer_in_Alternating_Mar/.agent_comm",
  "assigned_at": "2025-08-03T21:10:54.624180",
  "status": "assigned"
}